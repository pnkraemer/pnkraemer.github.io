<!DOCTYPE html>
<html>

<head>
<link rel="shortcut icon" href="favicon-product-hunt.ico" />
<link rel="stylesheet" type="text/css" href="style.css">
<title>Peter Nicholas Kr&auml;mer</title>

<link href="https://fonts.googleapis.com/css?family=Oswald:200,300,400,500,700" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Roboto:200,300,400,500,700" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Raleway:300" rel="stylesheet">


</head>

<header> 
<div class = "leftText">
<ul><li><a href="news.html"> &#x2191</a></li> </ul>
 </div>
<nav>
<ul><a id="navBar"> </a>
<li><a href="news.html#navBar" class='active-page'>News</a></li>
<li><a href="research.html#navBar">Research</a></li>
<li><a href="about.html#navBar">About me</a></li>
<li><a href="contact.html#navBar">Contact</a></li>
<li><a href="media.html#navBar">Media</a></li>
</ul>
</nav> 

<div class="transbox">
<div class="headerTitle"> Peter Nicholas Kr&auml;mer </div>
<div class ="headerText"> 
    PhD Student Machine Learning // University of T&uuml;bingen </div></div>



</header>

<div class="wrapper">
<body>
<h1>
News
</h1>


<h2>
Sequential Monte Carlo Methods 2019
</h2>
<p>
This summer I will participate at the Department of Information Technology's course about sequential Monte Carlo methods which takes place August 26 - 30 at Uppsala University (<a href = "http://www.it.uu.se/research/systems_and_control/education/2019/smc">link</a>). 
</p>
<p>
<i> July 3, 2019 </i>
</p>


<h2>
PhD Announcement
</h2>
<p>
I am excited to announce that in September 2019 I will start a PhD in Machine Learning at the University of TÃ¼bingen under supervision of Prof. Dr. Philipp Hennig. We will be working on automated, data-driven inference for mechanical models as part of the <i>ADIMEM</i> project; see <a href = "https://fit.uni-tuebingen.de/Activity/Details?id=6097">here</a>. Stay tuned for more!
</p>
<p>
<i> July 3, 2019 </i>
</p>


<h2>
New slides and new abstract online!
</h2>
<p>
This afternoon I gave the final presentation concluding my MSc thesis about Gaussian processes and Uncertainty quantification. In this thesis, I analyse Gaussian process emulators for Bayesian inverse problems. See the <a href="media.html#navBar">media</a> page for an abstract as well as for the slides of my presentation. Some of the python scripts I used for simulations can be found on my GitHub page: see <a href="https://github.com/pnkraemer/gp-emulators", target = blank>here</a>. 
</p>
<p>
<i> May 17, 2019 </i>
</p>




<h2>
Spotlight: <i>Good Programming Practice</i>
</h2>
<p>
Something I have always been intrigued by is good programming practice. For instance, proper construction of classes, good variable naming or useful commenting.
<!--As a simple example, look at the following python definition of a Gaussian kernel function: </p>
<pre>def K(a, b, lam):
	A = np.linalg.norm(a-b)**2
	return np.exp(-A/lam)
</pre>
<p>
Compare it to this, cleaner version:
</p>
<pre>def gaussKernel(pt1, pt2, corrLength = 1.0):
	normOfDiff = np.linalg.norm(pt1-pt2)
	return np.exp(-normOfDiff**2 / corrLength)
</pre>
<p>
At least to me, the second version is a lot more descriptive.-->
A list of 30 basic rules is provided <a href="https://opensource.com/article/17/5/30-best-practices-software-development-and-testing" target="_blank">here</a> and a shorter list of 15 rules can be found <a href="https://code.tutsplus.com/tutorials/top-15-best-practices-for-writing-super-readable-code--net-8118" target="_blank">here</a>.

I am still learning about good practice. What is your experience?
</p>
<p>
<i> February 27, 2019 </i>
</p>





<h2>
Spotlight: <i>Probabilistic Numerics</i>
</h2>
<p>
Probabilistic numerics is a rather new field of research which can be seen as a statistical, or probabilistic, point of view on numerics. It exhibits many connections to Bayesian inference and uncertainty quantification.
The main idea is to view numerical algorithms as statistical estimators. This leads to several other reinterpretations, for example of discretisation errors. 
</p>
<p>
Basically, all foundational numerical procedures are being investigated from this new point of view: quadrature, numerical linear algebra, optimisation, differential equations and more.

A summary of ongoing research is provided on <a href="http://probabilistic-numerics.org/" target="_blank">probabilistic-numerics.org</a>.

</p>
<p>
<i> January 6, 2019 </i>
</p>






<h2>
New slides online!
</h2>
<p>
Today in the post-graduate seminar at the Institute for Numerical Simulation, I gave a presentation about Gaussian process emulators for expensive simulations. I surveyed Gaussian process emulators and applications to Bayesian inverse problems. Further, I show some ideas for experimental design. Find the slides on the  <a href="media.html#navBar">media</a> page!

</p>
<p>
<i> December 13, 2018 </i>
</p>

<h2>
New slides online!
</h2>
<p>
Yesterday I gave a presentation about Gaussian process approximations in Bayesian inverse problems. It was the first of three presentations I have to give about my MSc thesis. In this presentation I survey Gaussian processes and radial basis function interpolation, Bayesian inverse problems and Gaussian process approximations in Bayesian inverse problems. Find the slides on the  <a href="media.html#navBar">media</a> page!
</p>
<p>
<i> December 7, 2018 </i>
</p>


<h2>
We are back!
</h2>
<p>
The website is back with a new look, as of December 6, 2018! Thanks to <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>!
</p>
<p>
<i> December 6, 2018 </i>
</p>

<h2>
Spotlight: <i>Gaussian Processes and Reproducing Kernel Hilbert Spaces</i>
</h2>
<p>
One thing I am fascinated by is the connection between Gaussian process regression and deterministic techniques in reproducing kernel Hilbert spaces. For instance, the predictive mean in Gaussian process regression is nothing but the radial basis function interpolant from scattered data approximation.
There are nice surveys available, for example <a href="https://arxiv.org/abs/1807.02582" target="_blank">here</a> or <a href="https://www.cambridge.org/core/journals/european-journal-of-applied-mathematics/article/interpolation-of-spatial-data-a-stochastic-or-a-deterministic-problem/D1EA0D2A6379B7737FCA054F14172E7A" target="_blank">here</a> (a pdf of the second paper can be found <a href="http://num.math.uni-goettingen.de/schaback/research/papers/IoSD.pdf" target="_blank">here</a>).
</p>
<p>
<i> November 7, 2018 </i>
</p>

<h2>
New slides and notes online!
</h2>
<p>
From May to July I participated in a <a href="https://ins.uni-bonn.de/teachings/ss-2018-201-s5e1-graduate-seminar/" target="_blank">seminar about high-dimensional approximation and uncertainty quantification</a>. There, I gave a talk about stochastic collocation. I also typed up a summary of what I said in the talk. Find the slides and the notes on the  <a href="media.html#navBar">media</a> page!
</p>

<p>
<i> August 2, 2018 </i>
</p>

<h2>
New slides online!
</h2>
<p>
Last week I gave a presentation about the <a href="http://h2lib.org/" target="_blank">H2Lib</a> at a post-graduate seminar at the University of Bonn. The H2Lib is an open source software library for hierarchical matrices, also known as H-matrices, and H<sup>2</sup>-matrices. In the presentation I explain how to get started on the H2Lib and show some experiments with scattered data approximation.
Find the slides on the  <a href="media.html#navBar">media</a> page!
</p>
<p>
<i> May 22, 2018 </i>
</p>

<h2>
Spotlight: <i>Hierarchical Matrices</i>
</h2>
<p>
Hierarchical matrices can be used to approximate non-sparse matrices in near-linear complexity. The only requirement is that the matrix entries depend on a kernel function which exhibits a so-called <i>asymptotical smoothness</i>, like the Green kernel or the Gaussian kernel. They are very popular in boundary element methods and seem to gain popularity in finite element methods and Gaussian process techniques as well. 
</p>
<p>
I learned about hierarchical matrices at the <a href="https://www.csc.uni-kiel.de/de/veranstaltungen/wshmat2018" target="_blank">winter school on hierarchical matrices</a> in Kiel last February/March; they have <a href="https://www.mis.mpg.de/de/publications/andere-reihen/ln/lecturenote-2103.html" target="_blank">lecture notes</a> available. There are also many books about hierarchical matrices, for example <a href="https://www.springer.com/de/book/9783662473238" target="_blank"> this one </a> or <a href="https://www.springer.com/de/book/9783540771463" target="_blank">this one</a>.
</p>
<p>
<i> April 8, 2018 </i>
</p>

<h2>
Website started!
</h2>
<p>
Welcome to my website! Here, you can find some information about me, especially <a href="about.html#navBar">who I am</a> and how to <a href="contact.html#navBar">get in touch</a>. In the <a href="news.html#navBar">news</a> section, I will occasionally ramble about what I think is interesting. In the <a href="media.html#navBar">media</a> section, you can find slides of some of my presentations, among other things. Enjoy!
</p>
<p>
<i> March 15, 2018 </i>
</p>

</body>






<footer>


<div class ="right">
&copy; Peter Nicholas Kr&auml;mer, 2019
</div>
</footer>
</div>
</html> 

