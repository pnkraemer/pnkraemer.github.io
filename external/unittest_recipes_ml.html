<!DOCTYPE html>

<!--
I use full sentences for each bullet.
Use e.g. formulations such as "Test whether", "Test if", "Assert that" and the like.
Avoid "Check", I think that reads like brainstorming.
For corner cases, use question-like formulations such as "What should the (...) be if (...)" or "How should the algorithm behave if (...)".
Use imperative for the bullets.
If there is a comment by the author to be made, use "I" and not "we"
If something needs to be clarified, do it between section header and bullets in italic.
If there is something else to mention (e.g. notation for maths), do it afterwards but still befor ethe bullets, but not in italic.
Avoid brackets.
Use external links with the target=_blank key.
Use as few words as necessary. That makes reading easier.
-->

<html>

<head>
    <title>Unittests Recipes for ML</title>

    <!-- FOR MATHS -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="unittest_style.css">
</head>

<body>
    <header id="header">
        <h1>Unittests Recipes for ML</h1>
        <p>
            <a href="http://pnkraemer.github.io" target="_blank">by Nicholas Krämer</a>
        </p>
    </header>

    <main>
        <section id="start">
            <p>
                <b> We have to do it. </b> Despite the popularity of software libraries like pytorch, tensorflow, numpy/scipy or scikit-learn, sometimes we cannot get around writing our own implementations of basic machine learning (ML) algorithms. To ensure that these implementations are working as expected, we write unittests.
            </p>
            <p>
                <b> It is difficult. </b> Coming up with <i> meaningful </i> unittests for algorithms involving machine learning or computational statistics is tedious in the best and extremely difficult in the worst case. To make life easier, on this website I summarise recipes for meaningful unittests for a range of ML algorithms.
            </p>
            <p>
                <b> Some vital things are ignored. </b> Albeit central to meaningful unittesting, I do not mention implementation-specific tests. For example: ''I want my parameter to be a float, hence for any other input, the algorithm must raise an exception''). Of course, this does <i>not</i> mean that they are not to be considered for unittests.
            </p>
            <p>
                <b> Help the cause. </b> If you spot typos, errors, problems or have any suggestions, please do not hesitate to <a href="https://pnkraemer.github.io/sites/contact.html#navBar" target=_blank>get in touch</a>.
            </p>
        </section>

        <section id="contents">
            <h2>Contents</h2>
            <ol>
                <li><a href="#automatic_differention"> Automatic differentiation</a> </li>
                <li><a href="#local_minimisation"> (General) local minimisation</a> </li>
                <li><a href="#random_search_minimisation"> Random search minimisation</a> </li>
                <li><a href="#gradient_descent_minimisation"> Gradient descent minimisation</a> </li>
                <li><a href="#newton_minimisation"> Newton minimisation</a> </li>
                <li><a href="#metropolis_hastings"> (General) Metropolis-Hastings</a> </li>
                <li><a href="#langevin_proposals"> Metropolis-Hastings with Langevin proposals</a> </li>
                <li><a href="#hamiltonian_proposals"> Hamiltonian Monte Carlo </a> </li>
                <li><a href="#gibbs_sampling"> Gibbs sampling </a> </li>
                <li><a href="#principal_component_analysis"> Principal component analysis</a> </li>
                <li><a href="#numerical_integration"> Numerical integration</a> </li>
                <li><a href="#ordinary_differential_equations"> Ordinary differential equations</a> </li>
                <li><a href="#iteratively_solving_linear_systems"> Iteratively solving linear systems</a> </li>
                <li><a href="#covariance_functions"> Covariance functions</a> </li>
                <li><a href="#data_sets"> Data sets</a> </li>
                <li><a href="#supervised_learning"> Supervised learning</a> </li>
                <li><a href="#neural_networks"> Neural networks</a> </li>
                <li><a href="#singular_value_decomposition"> Singular value decomposition </a> </li>
                <li><a href="#regression"> Regression</a> </li>
                <li><a href="#probability_distributions"> Probability distributions</a> </li>
                <li><a href="#kalman_filter"> Kalman filter </a> </li>
                <li><a href="#gaussian_process_regression"> Gaussian process regression </a> </li>
                <li><a href="#preconditioners"> Preconditioners </a> </li>
                <li><a href="#acknowledgements"> Acknowledgements </a> </li>
            </ol>

            <section id="automatic_differention">
                <h2>Automatic differentiation</h2>
                <i> When I say ''derivative'' I mean derivatives, gradients or Jacobians. </i>
                <ul>
                    <li> Assert that the automatically computed derivative is close to a finite difference approximation of the derivative. For direction-based derivatives like gradients, make sure that this holds true for each possible direction. </li>
                    <li> Test if the Hessian of a convex function is positive definite. To this end, pick a convex function, e.g. \(f(x)=3x^8 + x^4\), sample evaluation points \(x_1, ..., x_N\) and compute the smallest eigenvalue of the Hessian at each \(x_i, ~i=1, ..., N\). Remember to keep \(N\) as small as necessary for speed reasons. </li>
                    <li>
                        <b> Potential corner cases:</b>
                        <ul>
                            <li> What happens if the function that is to be differentiated does not allow a derivative?</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section id="local_minimisation">
                <h2>(General) local minimisation</h2>
                <p>
                    <i> This section also applies to all minimisation algorithms below. Aspects unique to stochastic versions can be found <a href="#stochastic_minimisation"> here</a>. </i>
                </p>
                <p>
                    I consider steepest descent minimisation of a loss function \(\ell(x)\) with iterations of the form \(x_i \mapsto x_i + \alpha \delta_i\), \(\alpha > 0 \).
                </p>
                <ul>
                    <li> Test whether for a really small learning rate \(\alpha \) the objective function is smaller after the minimisation step. </li>
                    <li> Assert that only the variable over which you want to optimise changes with each iteration. </li>
                    <li> Test that the optimiser finds the minimum of a few testfunctions. See e.g. <a href="https://www.sfu.ca/~ssurjano/optimization.html" target=_blank> here </a> for an overview. Strictly speaking this may not be a unittest.
                        <li>
                            <b> Potential corner cases:</b>
                            <ul>
                                <li> What does the algorithm do if the learning rate \(\alpha\) is negative, zero or numerically zero, i.e. smaller than \( 10^{-16}\)?</li>
                            </ul>
                        </li>
                </ul>
            </section>

            <section id="random_search_minimisation">
                <h2>Random search minimisation</h2>

                <ul>
                    <li> Compute a few samples. Test that each loss is smaller than or equal to the previous one. </li>
                    <li> Choose a convex problem and compute a few samples. Assert that for iterations where the loss is strictly smaller than the previous loss, the iteration is strictly closer to the true minimum. </li>
                </ul>

            </section>
            <section id="gradient_descent_minimisation">
                <h2>Gradient descent minimisation</h2>
                <ul>
                    <li> Assert that for convex problems, the gradient always points towards the local minimum.</li>
                    <li>
                        <b> Potential corner cases:</b>
                        <ul>
                            <li> What does the algorithm do if there is no gradient?</li>
                        </ul>
                    </li>

                </ul>
            </section>

            <section id="newton_minimisation">
                <h2>Newton minimisation</h2>
                <ul>
                    <li> Test whether for quadratic objective functions and learning rate \(\alpha=1\), Newton minimisation converges in a single step
                        <li>
                            <b> Potential corner cases:</b>
                            <ul>
                                <li> What does the algorithm do if there is no Hessian?</li>
                                <li> How does the algorithm react to singular Hessians?</li>
                            </ul>
                        </li>

                </ul>
            </section>

            <section id="stochastic_minimisation">
                <h2>Stochastic minimisation</h2>
                <i> Here I summarise things unique to the stochastic versions of the minimisation methods mentioned above.</i>
                <ul>
                    <li> Test that with maximal batch size it coincides with classic minimisation methods.</li>
                    <li>
                        <b> Potential corner cases:</b>
                        <ul>
                            <li> What does the algorithm do for batch-size zero? </li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section id="metropolis_hastings">
                <h2> (General) Metropolis-Hastings</h2>
                <i> Additional to the points mentioned below, one can use many different statistical hypothesis tests to assert whether the resulting samples have the desired invariant distribution.
                Bear in mind that for a meaningful qq-plot one may need many samples which can make the automated testing slow. </i>
                <ul>
                    <li> Test that proposals in regions with higher probability are always accepted. </li>
                    <li> Test that samples from a distribution with known CDF the qq-plot should be a straight line. Try this with multiple distributions and aim to include distributions with different supports. </li>
                    <li>
                        <b> Potential corner cases:</b>
                        <ul>
                            <li> What does the sampler do if some regions have zero probability? This can be investigated by sampling from e.g. the exponential or inverse gamma distribution.</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section id="langevin_proposals">
                <h2>Metropolis-Hastings with Langevin proposals</h2>
                <i> This is usually referred to as Metropolis-adjusted Langevin algorithm (MALA). </i>
                <ul>
                    <li> Assert that MALA is equivalent to Hamiltonian Monte Carlo (HMC) if HMC uses only one leapfrog step. Be careful: depending on your implementation the proposal widths may have to be adapted via either \(\sqrt{2 \rho} \leftrightarrow \rho \leftrightarrow \rho^2/2 \). </li>
                </ul>
            </section>

            <section id="hamiltonian_proposals">
                <h2>Hamiltonian Monte Carlo</h2>
                <i> The presentation is restricted to the case where the Hamiltonian dynamics are simulated with the leapfrog integrator. This can be phrased as an instance of a Metropolis-Hastings algorithm and the MH unittest-suggestions mentioned <a href="#metropolis_hastings">above</a> apply. </i>
                <ul>
                    <li> Assert that HMC with one leapfrog step is equivalent to MALA; see <a href="#langevin_proposals">above</a>. Be careful: depending on your implementation the proposal widths may have to be adapted via either \(\sqrt{2 \rho} \leftrightarrow \rho \leftrightarrow \rho^2/2 \)) </li>
                </ul>
            </section>

            <section id="gibbs_sampling">
                <h2>Gibbs sampling</h2>
                <ul>
                    <li> Test that the conditional distribution is consistent with the joint distribution. To this end, generate some samples and test Equation 1 on page 4 of <a href="https://arxiv.org/pdf/1412.5218.pdf" target=_blank>this paper</a>. </li>
                </ul>
            </section>

            <section id="principal_component_analysis">
                <h2> Principal component analysis</h2>
                <ul>
                    <li> Compute the reconstruction error of using the maximal number of principal components. Assert that it is zero. </li>
                    <li> Assert that only the largest principal components are chosen. This is important as some library functions sort singular values and others do not. </li>
                    <li> Assert that the eigenvalues are indeed eigenvalues to the eigenvectors. </li>
                    <li> Assert that the PCA algorithm at least raises a warning if the data is not normalised to some extend.</li>
                    <li>
                        <b> Potential corner cases:</b>
                        <ul>
                            <li> What happens if the covariance matrix is singular? </li>
                            <li> What does the algorithm do if the number of principal components is zero? </li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section id="numerical_integration">
                <h2> Numerical integration</h2>
                <ul>
                    <li> Numerical integration rules based on polynomial interpolation have certain degrees of exactness, say \(N\). Test that this degree is satisfied: the error of a polynomial of degree \(\leq N\) should be roughly machine precision and the error of a polynomial of degree \(\geq N + 1\) should be larger. </li>
                    <li> Assert that integrals of probability density functions are close to 1. Try distributions where the PDFs have different supports. For the Gaussian distribution, keep the <a href="https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule" target=_blank> 68-95-99.7</a> rule in mind when restricting the domain of integration to a compact interval. </li>
                    <li>
                        <b> Potential corner cases:</b>
                        <ul>
                            <li> What is the result if the interval of integration consists of only a single point? </li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section id="ordinary_differential_equations">
                <h2> Ordinary differential equations</h2>
                <ul>
                    <li> Test that the approximation error is small when comparing the approximate solution of an ODE with a known solution to the true solution. One option can be the <a href="https://en.wikipedia.org/wiki/Logistic_function#Logistic_differential_equation" target=_blank> logistic differential equation</a>. Assert that the error gets smaller as the step size gets smaller.</li>
                    <li> Assert that the approximate solution to a periodic system is perodic. For instance, test on the Lotka-Volterra equations or the FitzHugh-Nagumo model. For speed reasons, configure the parameters such that the period is short. </li>
                    <li> Compute the first few iterations of an ODE solver by hand and compare the result of the algorithm to your own calculations. Test that both coincide.
                        <li>
                            <b> Potential corner cases:</b>
                            <ul>
                                <li> How does the algorithm react to the time interval consisting of a single point? </li>
                            </ul>
                        </li>
                </ul>
            </section>

            <section id="iteratively_solving_linear_systems">
                <h2> Iteratively solving linear systems</h2>
                <ul>
                    <li> Test that the iterative solver achieves the desired relative or absolute solution accuracy used as an input to the algorithm. </li>
                    <li> Assert that dimension mismatches between matrix and right hand side are detected. </li>
                    <li> Test if the initialisation with true solution converges in zero, respectively a single step. </li>
                    <li> Test that the solution of a system based on a matrix with condition number 1, e.g. the identity matrix, the solution is returned in a single step</li>
                    <li>
                        <b> Potential corner cases:</b>
                        <ul>
                            <li> What does the solver do if matrix, right hand side or initial guess are empty arrays? </li>
                            <li> What happens if the maximum number of iterations is set to zero? </li>
                            <li> What happens if the desired accuracy is set to zero? </li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section id="covariance_functions">
                <h2> Covariance functions</h2>
                <i> I restrict this presentation to covariance functions that are reproducing kernels of a reproducing kernel Hilbert space. </i>
                <ul>
                    <li> Test that covariance matrices are symmetric</li>
                    <li> Assert that covariance matrices are positive definite. To this end, compute the minimal eigenvalue and make sure they are positive. </li>
                    <li> Compare evaluations of your covariance function against hand-computed values. </li>
                </ul>
            </section>

            <section id="data_sets">
                <h2> Data sets</h2>
                <i> This section is concerned with ``debugging'' data sets, i.e. with finding anomalies in data sets which might lead to unexpected behaviour of ML algorithms. </i>
                <ul>
                    <li> Look up which data types to expect in the data set and look for anomalies. </li>
                    <li> Find and deal with missing values and NaNs. </li>
                    <li> Look for surprisingly large variances among the data within each category. Sometimes, data sets contain values like -9999 instead of a NaN. If you find a "suspicious" category, investigate further by computing summary statistics and plotting a histogram.</li>
                    <li> Sometimes it is important to identify whether the data is normalised. If it is supposed to shifted to [-1, 1], test whether min and max are within that range. If it is normalised via \(X \mapsto (X-\text{mean}(X))/\text{std(X)}\), test whether the mean is zero and the standard deviation is 1. </li>
                </ul>
            </section>

            <section id="supervised_learning">
                <h2> Supervised learning</h2>
                <ul>
                    <li> Assert that test set and training set have similar distributions. To this end, compute and compare summary statistics like mean, (co)variance, median or others.</li>
                    <li> Test that the loss function value decreases after a few iterations. This is taken from <a href="https://developers.google.com/machine-learning/testing-debugging/pipeline/deploying" target=_blank>here</a>.</li>
                    <li>
                        <b> Potential corner cases:</b>
                        <ul>
                            <li> What happens if there is only a test set or only a training set?</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section id="neural_networks">
                <h2> Neural networks </h2>
                <i> This section would likely benefit the most from reader input. </i>
                <ul>
                    <li> Assert that on a small selection of data the model overfits after only few epochs? This is taken from <a href="https://mc.ai/some-techniques-for-unit-testing-machine-learning-models/" tarket=blank>here</a>.</li>
                </ul>
            </section>

            <section id="singular_value_decomposition">
                <h2> Singular value decomposition</h2> The notation assumes a singular value decomposition (SVD), \(M = U S V^\top\).
                <ul>
                    <li> Assert that if \(M\) is square, symmetric and positive definite, the singular values are eigenvalues and the columns of \(U\) and \(V\) are eigenvectors.</li>
                    <li> Test that identities \[M^\top M = V (S^\top S) V^\top, \quad M M^\top = U (S S^\top) U^\top,\] hold true. </li>
                    <li> Test if the matrices \(U\) and \(V\) are orthogonal. That is, assert that they satisfy \[U^\top U = U U^\top = I, \quad V^\top V = V V^\top = I,\] where \(I\) is an identity matrix of respective size. </li>
                    <li> Test that if the covariance matrix is a vector \(M \in \mathbb{R}^{m \times 1} \), \(U\) is a scalar equal to 1. Test the same for \(M^\top\) and \(V\) as well.
                        <li>
                            <b> Potential corner cases:</b>
                            <ul>
                                <li> What does the SVD do if the matrix \(M\) is an empty array? </li>
                                <li> What does the SVD do if the matrix \(M\) is a scalar? </li>
                            </ul>
                        </li>
                </ul>
            </section>

            <section id="regression">
                <h2> Regression</h2>
                <ul>
                    <li> Linear regression: Test that if the data are samples from a linear function without noise, the function should be recovered exactly with error equal to zero.</li>
                </ul>
            </section>

            <section id="probability_distributions">
                <h2> Probability distributions</h2>
                <i> What I mean by unittesting probability distributions is verifying that sampling methods and evaluations of probability density functions are correctly implemented.
                </i>
                <ul>
                    <li> Draw a significant amount of samples and test that the empirical moments match the known moments, if available. For speed reasons, consider using as few samples as necessary.</li>
                    <li> Use the same qq-plot tests I mentioned above under <a href="#metropolis_hastings">Metropolis-Hastings</a>.
                    </li>
                    <li>
                        <b> Potential corner cases:</b>
                        <ul>
                            <li> What happens if the distribution does not support sampling? This can make it hard to unittest. Possible solutions are comparing integrals of the PDF to external implementations of the CDF or to use Metropolis-Hastings methods to generate samples. When comparing to e.g. scipy.stats' cumulative densitiy functions, be aware that scipy.stats often approximates these via quadrature which makes them inexact.</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section id="kalman_filter">
                <h2> Kalman filter</h2>
                <ul>
                    <li> Test that if the datastream consists of noise-free samples from the same state space model that the Kalman filter uses, the filtering result is exact. In other words, the Kalman gain and correction factor must be zero.</li>
                    <li>
                        <b> Potential corner cases:</b>
                        <ul>
                            <li> What does the filter do if the data consists of an empty array?</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section id="gaussian_process_regression">
                <h2> Gaussian process regression</h2>
                <ul>
                    <li> For interpolation, which uses exact data: Test that after conditioning the Gaussian process, the variance at the datapoints are exactly zero. </li>
                    <li> For regression, which uses noisy data: Test that after conditioning the Gaussian process, the variance at the datapoints are proportional to the standard deviation of the data point. </li>
                    <li> To test whether the covariance matrix is accidentally computed as \(K + sI\) instead of \(K + s^2I\): choose a small \(s\) which is large enough such that \(s^2\) is within machine precision, e.g. \(s=10^{-7}\), and test if after conditioning the process, the average deviation from the data (see bullet above) is proportional to \(s=10^{-7}\) or \(s=10^{-14}\).
                    </li>
                    <li> An alternative option to test the case that the previous bullet describes is to choose a large \(s\), e.g. 10 or 100 and investigate whether the smallest eigenvalue of \(K + sI\), respectively \(K + s^2I\) is closer to 100 or to 10000.</li>
                    <li> If error estimates for the covariance function are available---which is e.g. the case for Matern class covariances or Wendland kernels---test that the error estimate is roughly fulfulled. That is, for error rate e.g. \(h^2\), where \(h\) is the fill-distance of the pointset, test if for a pointset 4 times as large, the error is roughly a 16th of the previous error. It is unlikely to obtain an exact 16th so if you find something in between a 10th and a 22nd, the test should pass. </li>
                    <li>
                        <b> Potential corner cases:</b>
                        <ul>
                            <li> What happens if the covariance matrix \(K\) is singular?</li>
                            <li> What happens if the data consists of an empty array?</li>
                            <li> What does the conditioning do if the measurements are not taken at distinct timepoints/spacepoints?</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section id="preconditioners">
                <h2> Preconditioners</h2>
                <i> In this section I treat all kinds of preconditioning: numerical linear algebra, optimisation, Markov chain Monte Carlo, and more. </i>
                <ul>
                    <li> Test that preconditioning with the identity matrix yields the same results as the unpreconditioned version. </li>
                    <li> Assert that preconditioning with the "true solution"---if available---yields the solution straightaway. For example, use the true inverse for this test in the setting of linear algebra. </li>
                </ul>
            </section>

            <section id="acknowledgements">
                <h2> Acknowledgements </h2>
                <p>
                    The design of this website is inspired by <a href="https://jgthms.com/web-design-in-4-minutes/" target=_blank>web design in 4 minutes</a>.
                </p>
            </section>

            <footer>
                &copy; Nicholas Krämer, 2020.
            </footer>
    </main>
</body>

</html>